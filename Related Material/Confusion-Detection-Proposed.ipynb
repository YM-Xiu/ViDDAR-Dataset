{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Detection - Proposed Method with GPT-4o\n",
    "\n",
    "detection accuracy = 82.46%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import sys\n",
    "import base64\n",
    "import requests\n",
    "from PIL import Image\n",
    "import io\n",
    "import json\n",
    "import uvicorn\n",
    "from fastapi import FastAPI, File, UploadFile, Form\n",
    "from fastapi.responses import FileResponse, StreamingResponse, JSONResponse\n",
    "from PIL import Image, ImageOps, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import io\n",
    "import subprocess\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from lang_sam import LangSAM\n",
    "import torch\n",
    "import cv2\n",
    "import base64\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import queue\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# account 1: google login\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
    "api_key = '...'\n",
    "\n",
    "question = \"\"\"Here are two images. The first one is a raw image, the second one is an augmented image, created by adding some virtual content to the space.\n",
    "Please answer the following questions:\n",
    "1.What is the virtual content in the augmented image?\n",
    "2.What \"key object\" is interacting with the virtual content? Avoid general terms like “table surface” or “environment”.\n",
    "3.Is the virtual content accurately aligned to object, without significant gap? Answer yes or no, then explain why.\n",
    "4.Does the virtual content have a relatively high quality of texture? Answer yes or no, then explain why.\n",
    "5.Do you think the interaction will make users to believe the \"key object\" has some false functionality or features it doesn’t have, \n",
    "or lost some true functionality it actually has? Think creatively, don't say \"no\" easily, only say no if the combination has no specific semantic relation. Answer yes or no, then explain why.\n",
    "6.If you answered “yes” in all question 3, 4 and 5, you must say “yes”. Otherwise you say “no”.\"\"\"\n",
    "\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "\n",
    "result_csv = 'confusion_gpt.csv'\n",
    "response_txt = 'response_gpt.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_last_occurrence(input_string):\n",
    "    # Split the string into words\n",
    "    words = input_string.lower().split()\n",
    "\n",
    "    # Initialize variables to store the last positions of \"yes\" and \"no\"\n",
    "    last_yes = -1\n",
    "    last_no = -1\n",
    "\n",
    "    # Iterate through the words and track the positions of \"yes\" and \"no\"\n",
    "    for i, word in enumerate(words):\n",
    "        if word == \"yes\" or word == \"Yes\" or word == \"Yes.\" or word == \"yes.\" or word == \"Yes,\" or word == \"yes,\" or word == \"**Yes**\" or word == \"**yes**\":\n",
    "            last_yes = i\n",
    "        elif word == \"no\" or word == \"No\" or word == \"No.\" or word == \"no.\" or word == \"No,\" or word == \"no,\" or word == \"**No**\" or word == \"**no**\":\n",
    "            last_no = i\n",
    "\n",
    "    # Compare the positions to determine which appeared last\n",
    "    if last_yes > last_no:\n",
    "        return 1\n",
    "    elif last_no > last_yes:\n",
    "        return 0\n",
    "    else:\n",
    "        return 'N/A'  # In case neither \"yes\" nor \"no\" are found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Acc: 94/114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_644585/2194542101.py:114: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df_result[3][1:] = df_result[3][1:].astype(int)\n"
     ]
    }
   ],
   "source": [
    "# read the csv file\n",
    "df = pd.read_csv('confusion_dataset.csv')\n",
    "\n",
    "ar_folder = '.../confusion data/ar'\n",
    "raw_folder = '.../confusion data/raw'\n",
    "\n",
    "# # clear the txt\n",
    "# with open(response_txt, 'w') as f:\n",
    "#     f.write('')\n",
    "    \n",
    "start_point = 0\n",
    "\n",
    "for i in range(start_point, len(df)):\n",
    "    ar_img_name = ar_folder + '/' + str(i+1) + '_ar.png'\n",
    "    raw_img_name = raw_folder + '/' + str(i+1) + '_raw.png'\n",
    "    \n",
    "    ar_img = encode_image(ar_img_name)\n",
    "    raw_img = encode_image(raw_img_name)\n",
    "    \n",
    "    label = df.loc[i, 'confusion']\n",
    "    \n",
    "    payload = {\n",
    "        # ! This is the model for GPT-4 Vision\n",
    "        # \"model\": \"gpt-4-vision-preview\",\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": question\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{raw_img}\",\n",
    "                            \"detail\": \"high\"\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{ar_img}\",\n",
    "                            \"detail\": \"high\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 600\n",
    "    }\n",
    "    \n",
    "    bad_response = True\n",
    "    bad_response_list = ['sorry']\n",
    "    \n",
    "    while bad_response:\n",
    "    \n",
    "        # trial += 1\n",
    "        # print(f'Trial {trial}')\n",
    "\n",
    "        response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "        \n",
    "        # ! if there's no 'error' in the keys:\n",
    "        if 'error' in response.json().keys():\n",
    "            print('Error!')\n",
    "            print(response.json())\n",
    "            continue\n",
    "        text_response = response.json()['choices'][0]['message']['content']\n",
    "        \n",
    "        # remove \" and ' in the response\n",
    "        text_response = text_response.replace('\\\"', '')\n",
    "        text_response = text_response.replace(\"\\'\", '')\n",
    "        \n",
    "        if any(bad in text_response for bad in bad_response_list) == False:\n",
    "            bad_response = False\n",
    "            break\n",
    "        \n",
    "    # remove \\n, , in the response\n",
    "    text_response = text_response.replace('\\n', ' ')\n",
    "    text_response = text_response.replace(',', ' ')\n",
    "    \n",
    "    # write the response to a txt file\n",
    "    with open(response_txt, 'a') as f:\n",
    "        f.write(f'File {i+1}: {text_response}\\n')\n",
    "    \n",
    "    \n",
    "    # check \"yes\" or \"no\" that appear in the last\n",
    "    prediction = check_last_occurrence(text_response)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # claculate score\n",
    "    score = 0\n",
    "    if prediction == 'N/A':\n",
    "        score = 0\n",
    "    elif prediction == label:\n",
    "        score = 1\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "        \n",
    "        \n",
    "    # write the response to another csv file\n",
    "    with open(result_csv, 'a') as f:\n",
    "        f.write(f'{i+1},{label},{prediction},{score}\\n')\n",
    "        \n",
    "    # if i == start_point + 7:\n",
    "    #     break\n",
    "    \n",
    "print('Done!')\n",
    "# read the result csv and calculate the score\n",
    "df_result = pd.read_csv(result_csv, header=None)\n",
    "df_result[3][1:] = df_result[3][1:].astype(int)\n",
    "total_score = df_result[3][1:].sum()\n",
    "print(f'Acc: {total_score}/{len(df_result)-1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 94/114 = 0.8245614035087719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_644585/2392947878.py:2: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df_result[3][1:] = df_result[3][1:].astype(int)\n"
     ]
    }
   ],
   "source": [
    "df_result = pd.read_csv(result_csv, header=None)\n",
    "df_result[3][1:] = df_result[3][1:].astype(int)\n",
    "total_score = df_result[3][1:].sum()\n",
    "print(f'Acc: {total_score}/{len(df_result)-1} = {total_score/(len(df_result)-1)}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangSAM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
